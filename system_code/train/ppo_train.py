import os
import copy
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

import gymnasium as gym
import numpy as np
import pandas as pd
from gymnasium import spaces
from datetime import datetime
from sklearn.preprocessing import StandardScaler

from system_code.core.modules.print_system import PrintSystem
from system_code.core.modules.position import HoldingPeriod, HoldingGroup
from stable_baselines3.common.callbacks import BaseCallback
from system_code.core.config import Config


class TrainingLogger(BaseCallback):
    def __init__(self, verbose=1):
        super(TrainingLogger, self).__init__(verbose)

    def _on_step(self) -> bool:
        if self.n_calls % 100 == 0:  # æ¯ 100 ä¸ª step è®°å½•ä¸€æ¬¡
            mean_reward = np.mean(self.locals["rewards"])  # å½“å‰å¹³å‡å¥–åŠ±
            loss = self.model.logger.name_to_value.get("train/loss", None)
            entropy = self.model.logger.name_to_value.get("train/entropy_loss", None)

            print(f"ğŸ”¹ Step: {self.n_calls}, Mean Reward: {mean_reward:.4f}, Loss: {loss}, Entropy: {entropy}")
        return True  # ç»§ç»­è®­ç»ƒ


class HoldingPeriodEnv(gym.Env):
    """
    è‡ªå®šä¹‰ç¯å¢ƒ:
      - è¾“å…¥: ä¸€ä¸ª HoldingPeriod å¯¹è±¡ (åŒ…å«äº†å¤šæ ¹Kçº¿è¡Œæƒ…)
      - æ¯ä¸ª step:
          action=0 => ç»§ç»­æŒä»“
          action=1 => å¹³ä»“(episodeç»“æŸ)
      - episode åœ¨ä»¥ä¸‹æƒ…å†µç»“æŸ:
          1. åŠ¨ä½œä¸º1(å¹³ä»“)
          2. åˆ°è¾¾HoldingPeriodæœ€åä¸€æ ¹bar

    """
    def __init__(self, holding_period, fee_rate=0.0005):
        super(HoldingPeriodEnv, self).__init__()

        self.hp = holding_period
        self.fee_rate = fee_rate

        # å°†è¡Œæƒ…æ•°æ®å–å‡ºæ¥ï¼Œåé¢stepé€ä¸ªå–å€¼
        self.data = self.hp.data.reset_index(drop=True)
        self.n_bars = len(self.data)

        # è§‚å¯Ÿç©ºé—´
        # è¿™é‡Œå…ˆæ‰¾ä¸€ä¸‹æœ‰å“ªäº›åˆ—å¯ç”¨:
        drop_cols = [
            'inst_id_x', 'inst_id_y', 'ts','taker_sell', 'taker_buy', 'open_interest',
            'elite_long_short_ratio', 'elite_position_long_short_ratio',
            'all_long_short_ratio',
            'bullCondition', 'bearCondition', 'bullCondition_shift1',
            'bearCondition_shift1', 'buySignalRaw', 'sellSignalRaw',
            'buySignal', 'sellSignal', 'longFilter', 'shortFilter'
        ]
        self.data = self.data.drop(columns=drop_cols, errors='ignore')
        self.feature_columns = self.data.columns
        # æ ‡å‡†åŒ–å¤„ç†data
        # scaler = StandardScaler()
        # self.data = pd.DataFrame(scaler.fit_transform(self.data), columns=self.data.columns)


        # gym éœ€è¦å®šä¹‰ observation_spaceï¼Œç”¨äºæè¿°çŠ¶æ€ç»´åº¦ã€èŒƒå›´ç­‰
        # è¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼ŒæŠŠæ‰€æœ‰ feature åšæˆä¸€ä¸ªå‘é‡
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(len(self.feature_columns),), dtype=np.float32
        )

        # åŠ¨ä½œç©ºé—´: 0=ç»§ç»­æŒæœ‰, 1=å¹³ä»“
        self.action_space = spaces.Discrete(2)

        # è®°å½•ç¯å¢ƒåœ¨episodeä¸­çš„çŠ¶æ€
        self.current_step = 0
        # æŒä»“æœŸé—´å®æ—¶è¯„ä¼°ï¼Œæœ€å¤§æµ®äº
        self.max_unrealized_loss = 0.0
        # è¿›åœºä»·æ ¼: è¿™é‡Œå°±æ‹¿ HoldingPeriod è¿›åœºæ—¶çš„ entry_price
        self.entry_price = self.hp.entry_price
        # æ æ†
        self.leverage = self.hp.leverage

    def reset(self, **kwargs):
        """
        åœ¨ä¸€ä¸ªepisodeå¼€å§‹æ—¶ (å³ä¸€ä¸ªHoldingPeriodå¼€å§‹æ—¶), å°†çŠ¶æ€ç½®0
        :param **kwargs:
        """
        self.current_step = 0
        # é‡ç½®æœ€å¤§æµ®äº (è‹¥è¦ç”¨ hp å†…éƒ¨çš„å·²è®¡ç®—å€¼, å¯ä»¥ç›´æ¥æ‹¿ self.hp.max_unrealized_loss)
        self.max_unrealized_loss = 0.0
        info = {}

        return self._get_observation(), info

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œ
        """
        done = False
        reward = 0.0

        # å…ˆæ ¹æ®å½“å‰barçš„ä¿¡æ¯æ¥æ›´æ–° max_unrealized_loss
        # unrealized PnL = (å½“å‰ä»·æ ¼ - entry_price) / entry_price * leverage (å¤šå¤´æƒ…å†µ)
        # å¦‚æœæ˜¯ç©ºå¤´å¯æ”¹é€»è¾‘ã€‚ä¹Ÿå¯ä»¥æ‹¿ HoldingPeriod å†…ç½®çš„ logic.
        row = self.data.iloc[self.current_step]
        current_price = row['close']
        if self.hp.side == 'long':
            unrealized_pnl = self.leverage * ((current_price - self.entry_price) / self.entry_price)
        else:
            unrealized_pnl = self.leverage * ((self.entry_price - current_price) / self.entry_price)

        # æœ€å¤§æµ®äº, å…¶å®å°±æ˜¯æœ€å°çš„"unrealized_pnl"ä¸0çš„å·®å€¼, æˆ–è€… negative part
        # ä¸ºæ¼”ç¤ºç®€å•, è¿™é‡Œå®šä¹‰:
        if unrealized_pnl < 0:
            # negative part
            if unrealized_pnl < self.max_unrealized_loss:
                self.max_unrealized_loss = unrealized_pnl

        # åˆ¤æ–­æ˜¯å¦è¦å¹³ä»“ or åˆ°è¾¾æœ€åä¸€ä¸ªbar
        if action == 1 or self.current_step == (self.n_bars - 1):
            # done
            done = True

            # è®¡ç®—æœ€ç»ˆæ”¶ç›Š
            final_price = row['close']
            if self.hp.side == 'long':
                final_profit = self.leverage * ((final_price - self.entry_price) / self.entry_price)
            else:
                final_profit = self.leverage * ((self.entry_price - final_price) / self.entry_price)

            # æ‰£é™¤æ‰‹ç»­è´¹ (è¿›åœº+å‡ºåœº)
            # ä¾‹å¦‚: final_profit_fee = (1 - fee_rate) * (1 + final_profit) * (1 - fee_rate) - 1
            # è¿™é‡Œç®€å•åŒ–å¤„ç†
            final_profit_fee = (1 - self.fee_rate) * (1 + final_profit) * (1 - self.fee_rate) - 1

            # æŒä»“ bar æ•°
            holding_bars = self.current_step + 1  # ä»0æ•°èµ·ï¼Œè¦+1
            if self.max_unrealized_loss == 0:
                # å¦‚æœä»æœªæµ®äºï¼Œå¯ä»¥ç»™äºˆæ›´å¤§åŠ æˆ or ç›´æ¥è§†ä¸º final_profit
                drawdown_ratio = final_profit_fee
            else:
                # final_profit_fee / abs(self.max_unrealized_loss)
                drawdown_ratio = final_profit_fee / abs(self.max_unrealized_loss)

            # å°†ä¸‰ä¸ªè¦ç´ ç®€å•ç›¸åŠ 
            # 1) final_profit_fee
            # 2) final_profit_fee / holding_bars
            # 3) drawdown_ratio
            reward = self.compute_reward(final_profit_fee, holding_bars, self.max_unrealized_loss)

        # å¦‚æœæ²¡æœ‰ç»“æŸï¼Œè¿˜è¦å¾€åèµ°ä¸€æ­¥
        self.current_step += 1
        if self.current_step >= self.n_bars:
            self.current_step = self.n_bars - 1  # é˜²æ­¢è¶Šç•Œ

        # ç»„è£…ä¸‹ä¸€ä¸ª observation
        obs = self._get_observation()
        info = {}

        return obs, reward, done, False, info

    def _get_observation(self):
        """
        è¿”å›å½“å‰barçš„ç‰¹å¾å‘é‡
        """
        row = self.data.iloc[self.current_step]
        obs = row[self.feature_columns].values.astype(np.float32)

        return obs

    @staticmethod
    def compute_reward(final_profit_fee, holding_bars, max_unrealized_loss, max_hold_bars=288):
        """
        è®¡ç®—å¥–åŠ±å‡½æ•°ï¼Œä¼˜åŒ–å¥–åŠ±å€¼çš„å°ºåº¦ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šã€‚
        """
        # 1. å½’ä¸€åŒ–æ”¶ç›Š: [-0.2, 0.2] -> [-1, 1]
        scaled_profit = final_profit_fee * 10

        # 2. å•ä½æ—¶é—´æ”¶ç›Šï¼Œå½’ä¸€åŒ–æŒä»“æ—¶é—´å½±å“
        if final_profit_fee > 0:
            scaled_time_profit = final_profit_fee * 10 / holding_bars
        # 2.1 è€ƒè™‘æƒ…å†µï¼šç›¸åŒäºæŸï¼Œä½†æ˜¯æŒä»“æ—¶é—´æ›´é•¿ï¼Œåº”è¯¥æ›´å·®
        else:
            scaled_time_profit = final_profit_fee * 10 / holding_bars * max_hold_bars

        # 3. æ”¶ç›Š/æœ€å¤§æµ®äºæ¯”å€¼ï¼Œé¿å…æ•°å€¼æç«¯
        drawdown_ratio = final_profit_fee / (abs(max_unrealized_loss) + 1e-6)
        drawdown_ratio = np.clip(drawdown_ratio, -2, 2)  # é™åˆ¶èŒƒå›´

        # 4. ç»„åˆå¥–åŠ±ï¼Œå¹¶ä½¿ç”¨ tanh é™åˆ¶æ•´ä½“èŒƒå›´
        a = 3 / 5
        b = 1 / 5
        c = 1 / 5
        reward = np.tanh(a * scaled_profit +
                         b * scaled_time_profit +
                         c * drawdown_ratio)

        return reward


class TrainingModule(PrintSystem):
    def __init__(
            self,
            bar: str,
            inst_id: str,
            train_begin: datetime,
            train_end: datetime,
            test_begin: datetime,
            test_end: datetime,
            load_from_local=False,
            total_timesteps=250000
    ):
        super().__init__(bar, inst_id, train_begin, train_end)
        self.load_from_local = load_from_local
        self.total_timesteps = total_timesteps
        self.test_begin = test_begin
        self.test_end = test_end
        self.train_begin = train_begin
        self.train_end = train_end

    @staticmethod
    def train_model_for_group(holding_group, total_timesteps=200000):
        """
        é’ˆå¯¹ä¸€ä¸ªHoldingGroupï¼Œè®­ç»ƒä¸€ä¸ªRLæ¨¡å‹
        :param holding_group: HoldingGroupå¯¹è±¡
        :param total_timesteps: è®­ç»ƒçš„æ€»timesteps
        :return: è®­ç»ƒå¥½çš„æ¨¡å‹
        """

        # 1. ä¸ºHoldingGroupä¸­çš„æ¯ä¸ªHoldingPeriodåˆ›å»ºä¸€ä¸ªç¯å¢ƒ
        envs = []
        for hp in holding_group.holdings:
            # åˆ›å»ºç¯å¢ƒå®ä¾‹
            def _make_env(hp_local):
                # stable-baselines3 éœ€è¦æ— å‚å‡½æ•°ï¼Œæ‰€ä»¥å°ä¸€ä¸ªlambda
                return lambda: HoldingPeriodEnv(hp_local)

            envs.append(_make_env(hp))

        # 2. ç”¨ DummyVecEnv å¹¶è¡Œï¼ˆæˆ–ä¸²è¡Œï¼‰å°è£…è¿™äº›ç¯å¢ƒ
        # envs æ˜¯ä¸ªlist[callable], éœ€ä¼ ç»™ DummyVecEnv
        vec_env = DummyVecEnv(envs)

        # 3. åˆå§‹åŒ–PPOæ¨¡å‹
        # å¯ä»¥è‡ªå·±é€‰å–åˆé€‚çš„è¶…å‚æ•°
        model = PPO(
            policy='MlpPolicy',
            env=vec_env,
            verbose=1,
            n_steps=128,
            learning_rate=1e-4,
            gamma=0.99
        )

        # 4. å¼€å§‹è®­ç»ƒ
        model.learn(total_timesteps=total_timesteps, callback=TrainingLogger())

        return model

    def run_reinforcement_learning_system(self, holding_groups):
        """
        å‡è®¾æˆ‘ä»¬å·²ç»é€šè¿‡ open_module.open_position(data) å¾—åˆ°äº†å¤šä¸ªHoldingGroup
        ç„¶åå¯¹æ¯ä¸ªHoldingGroupå•ç‹¬è®­ç»ƒä¸€ä¸ªRLæ¨¡å‹
        """
        # 1. æ¯”å¦‚ä½ å·²ç»è·å¾—äº† holding_groups åˆ—è¡¨: List[HoldingGroup]
        # ä¾‹å¦‚:
        # holding_groups = self.open_module.open_position(data)
        # è¿™é‡Œæˆ‘ä»¬å…ˆå‡è®¾ holding_groups å·²ç»æœ‰äº†

        # 2. å¯¹æ¯ä¸ªHoldingGroupè®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
        group_models = []
        for i, group in enumerate(holding_groups):
            # è®­ç»ƒ
            model = self.train_model_for_group(group, total_timesteps=self.total_timesteps)
            group_models.append(model)

            # ä¿å­˜æ¨¡å‹
            model.save(os.path.join(Config.MODEL_DIR, f"{self.inst_id}_ppo_model_group_{i}.zip"))

        print("è®­ç»ƒå®Œæˆï¼Œå·²ç”Ÿæˆæ‰€æœ‰æ¨¡å‹")
        return group_models

    def inference_on_holding_period(self, model, hp: HoldingPeriod):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„ model å¯¹æŒ‡å®šçš„ holding_period è¿›è¡Œå†³ç­–
        """
        env = HoldingPeriodEnv(hp)
        obs, info = env.reset()
        done = False
        step_count = 0
        while not done:
            # 1) è·å–åŠ¨ä½œ
            try:
                action, _states = model.predict(obs, deterministic=True)
            except Exception as e:
                print(f"Error: {e}")
                action = 0

            # 2) ä¸ç¯å¢ƒäº¤äº’
            obs, reward, done, _, info = env.step(action)

            step_count += 1
            if done:
                hp.end = datetime.fromtimestamp(hp.data['ts'].iloc[step_count] / 1000)
                print(f"Episode done at step: {step_count}, reward={reward}")
                break

    def simulate_close_module(self, model, holding_group):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ¯ä¸ª HoldingGroup è¿›è¡Œå†³ç­–
        """
        for hp in holding_group.holdings:
            self.inference_on_holding_period(model, hp)

    def run(self):
        self.begin = self.test_begin
        self.end = self.test_end

        data = self.fetch_data()
        test_groups = self.open_module.open_position(data)

        if not self.load_from_local:
            self.begin = self.train_begin
            self.end = self.train_end

            data = self.fetch_data()
            train_groups = self.open_module.open_position(data)

            group_models = self.run_reinforcement_learning_system(train_groups)

            for model, group in zip(group_models, train_groups):
                self.simulate_close_module(model, group)
                group.update()

            df = self.backtest.run(train_groups)
            print(df.to_string())
            self.backtest.plot_all(save=True, inst_id=self.inst_id, to_add_title='train')
        else:
            group_models = []
            for i in range(len(test_groups)):
                model = PPO.load(os.path.join(Config.MODEL_DIR, f"{self.inst_id}_ppo_model_group_{i}.zip"))
                group_models.append(model)

            print("ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼Œå¼€å§‹æµ‹è¯•")

        original_test_groups = copy.deepcopy(test_groups)
        for model, group in zip(group_models, test_groups):
            self.simulate_close_module(model, group)
            group.update()

        df = self.backtest.run(test_groups + original_test_groups)
        print(df.to_string())
        self.backtest.plot_all(save=True, inst_id=self.inst_id, to_add_title='test')


if __name__ == '__main__':
    from datetime import timedelta


    time_delta = timedelta(days=365)
    train_begin = datetime(2023, 1, 4)
    train_end = datetime(2024, 2, 4)
    test_begin = train_begin + time_delta
    test_end = train_end + time_delta
    inst_id = 'DOGE-USDT-SWAP'
    bar = '5m'

    tm = TrainingModule(bar, inst_id, train_begin, train_end, test_begin, test_end, load_from_local=False, total_timesteps=5000000)
    tm.run()

